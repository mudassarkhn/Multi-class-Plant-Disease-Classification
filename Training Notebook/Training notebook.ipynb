{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "peIDZmT77gX3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUdnypqyYhio"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfRe34MnariR",
        "outputId": "3c465180-6f51-4032-af1b-d231a4efd7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d abdallahalidev/plantvillage-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XZ7dCnMazaQ",
        "outputId": "f81038c4-83d7-4afa-a925-8d054cf40a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/abdallahalidev/plantvillage-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading plantvillage-dataset.zip to /content\n",
            " 99% 2.02G/2.04G [00:30<00:00, 153MB/s]\n",
            "100% 2.04G/2.04G [00:30<00:00, 72.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynYeTUpDa5E-",
        "outputId": "aa31de92-9e10-4048-c15e-af2d731c863e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plantvillage-dataset.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the downloaded dataset"
      ],
      "metadata": {
        "id": "Hde3qBaK9QAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with ZipFile(\"plantvillage-dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "9MVt4OYIbezq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the path to dataset"
      ],
      "metadata": {
        "id": "w7gBoJSL9bEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/plantvillage dataset/color\""
      ],
      "metadata": {
        "id": "7KtazsqLcrhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define image size and batch size"
      ],
      "metadata": {
        "id": "WOdZhsKi9fZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "45nmRqX5dF5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tSXY5CicsNoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "BrWaXRKUeU0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageDataGenerator for data loading and augmentation"
      ],
      "metadata": {
        "id": "axkyEMMb9uBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
      ],
      "metadata": {
        "id": "iUJiZ_pvemts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data generator"
      ],
      "metadata": {
        "id": "sLkSVTop-FQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1iHQoGNen8q",
        "outputId": "03bdfa69-8d1f-4470-b873-6fa5492861de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43456 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation data generator"
      ],
      "metadata": {
        "id": "wLEc_kea-N_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVsi1KUteS4C",
        "outputId": "47cbf158-5bb5-425d-808e-1286b005e604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10849 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pre-trained MobileNetV2 model\n"
      ],
      "metadata": {
        "id": "EmtH_Z4k-YGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPnKErFpev6g",
        "outputId": "7a5b87e4-069d-4536-b1ee-6943d5a38c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlCGgWniA67M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the base model\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "dS85jYR3e05G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GlobalAveragePooling2D():** This layer reduces the spatial dimensions to a single value per feature map, effectively converting the 2D feature maps to 1D vectors.\n",
        "\n",
        "**Dense(128, activation='relu'):** Adds a fully connected layer with 128 units and ReLU activation.\n",
        "\n",
        "**Dropout(0.5):** Dropout layer for regularization, randomly deactivating 50% of neurons during training to prevent overfitting.\n",
        "\n",
        "**Dense(train_generator.num_classes, activation='softmax'):** Final dense layer, where train_generator.num_classes is the number of classes in your dataset."
      ],
      "metadata": {
        "id": "UZJP64ReAhI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom layers on top\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "8LUens8We4XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "cZVi1b_devCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=3,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
        ")\n"
      ],
      "metadata": {
        "id": "O_zJN_foe9cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating model...\")\n",
        "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um5l29W0b4i-",
        "outputId": "d3260795-f81e-4cb7-ce76-bcc8f170b6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\u001b[1m1247/1247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1576s\u001b[0m 1s/step - accuracy: 0.9417 - loss: 0.1695\n",
            "Validation Accuracy: 94.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZqWypNBVlgQ",
        "outputId": "ca084345-48c6-4341-9484-4c28a9ce2139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "id": "0jXnwZiurZgZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}